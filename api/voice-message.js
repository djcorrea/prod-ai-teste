import { auth, db } from './firebaseAdmin.js';
import cors from 'cors';

// Middleware CORS
const corsMiddleware = cors({
  origin: process.env.NODE_ENV === 'development' ? '*' : [
    'https://prod-ai-teste.vercel.app',
    /^https:\/\/prod-ai-teste-[a-z0-9\-]+\.vercel\.app$/
  ],
  methods: ['POST', 'OPTIONS'],
  credentials: true,
});

function runMiddleware(req, res, fn) {
  return new Promise((resolve, reject) => {
    fn(req, res, (result) => {
      if (result instanceof Error) return reject(result);
      return resolve(result);
    });
  });
}

// üé§ FUN√á√ÉO PRINCIPAL - VOICE MESSAGE HANDLER
export default async function handler(req, res) {
  console.log('üé§ Voice Message API chamada:', {
    method: req.method,
    timestamp: new Date().toISOString()
  });

  try {
    await runMiddleware(req, res, corsMiddleware);
  } catch (err) {
    console.error('CORS error:', err);
    return res.status(403).end();
  }

  if (req.method === 'OPTIONS') {
    return res.status(200).end();
  }

  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'M√©todo n√£o permitido' });
  }

  try {
    const { audioData, idToken } = req.body;

    if (!idToken) {
      return res.status(401).json({ error: 'Token de autentica√ß√£o necess√°rio' });
    }

    if (!audioData) {
      return res.status(400).json({ error: 'Dados de √°udio necess√°rios' });
    }

    // Verificar autentica√ß√£o
    let decoded;
    try {
      decoded = await auth.verifyIdToken(idToken);
    } catch (err) {
      return res.status(401).json({ error: 'Token inv√°lido' });
    }

    const uid = decoded.uid;
    const email = decoded.email;

    console.log('üéµ Processando √°udio para:', email);

    // 1. TRANSCRI√á√ÉO COM OPENAI WHISPER
    const transcription = await transcribeAudio(audioData);
    console.log('üìù Transcri√ß√£o:', transcription);

    // 2. AN√ÅLISE T√âCNICA DO √ÅUDIO
    const audioAnalysis = await analyzeAudioContent(audioData);
    console.log('üéõÔ∏è An√°lise:', audioAnalysis);

    // 3. GERAR RESPOSTA DA IA COM CONTEXTO COMPLETO
    const aiResponse = await generateVoiceResponse(transcription, audioAnalysis, uid);
    console.log('ü§ñ Resposta gerada para:', email);

    // 4. SALVAR NO FIRESTORE (OPCIONAL - PARA ANALYTICS)
    await saveVoiceInteraction(uid, {
      transcription,
      audioAnalysis,
      response: aiResponse,
      timestamp: new Date()
    });

    return res.status(200).json({
      success: true,
      transcription,
      audioAnalysis,
      aiResponse
    });

  } catch (error) {
    console.error('üí• Erro no Voice Message:', error);
    return res.status(500).json({ 
      error: 'Erro interno do servidor',
      details: process.env.NODE_ENV === 'development' ? error.message : 'Erro interno'
    });
  }
}

// üéß FUN√á√ÉO 1: TRANSCRI√á√ÉO COM WHISPER
async function transcribeAudio(audioBase64) {
  try {
    // Converter base64 para buffer
    const audioBuffer = Buffer.from(audioBase64, 'base64');
    
    // Criar FormData para Whisper API
    const formData = new FormData();
    formData.append('file', new Blob([audioBuffer], { type: 'audio/webm' }), 'audio.webm');
    formData.append('model', 'whisper-1');
    formData.append('language', 'pt'); // Portugu√™s

    const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
      },
      body: formData
    });

    if (!response.ok) {
      throw new Error(`OpenAI Whisper erro: ${response.status}`);
    }

    const result = await response.json();
    return result.text || '';

  } catch (error) {
    console.error('‚ùå Erro na transcri√ß√£o:', error);
    // Fallback: retornar mensagem gen√©rica se falhar
    return '√Åudio recebido - processando conte√∫do musical...';
  }
}

// üéõÔ∏è FUN√á√ÉO 2: AN√ÅLISE T√âCNICA DE √ÅUDIO
async function analyzeAudioContent(audioBase64) {
  try {
    // An√°lise b√°sica do buffer de √°udio
    const audioBuffer = Buffer.from(audioBase64, 'base64');
    
    const analysis = {
      fileSize: audioBuffer.length,
      fileSizeMB: (audioBuffer.length / (1024 * 1024)).toFixed(2),
      estimatedDuration: Math.ceil(audioBuffer.length / 44100), // Estimativa aproximada
      hasContent: audioBuffer.length > 1000,
      quality: audioBuffer.length > 100000 ? 'boa' : 'baixa',
      
      // An√°lise de conte√∫do baseada em caracter√≠sticas
      likelyContent: detectAudioContent(audioBuffer),
      
      // Sugest√µes t√©cnicas baseadas no tamanho/qualidade
      technicalSuggestions: generateTechnicalSuggestions(audioBuffer)
    };

    return analysis;

  } catch (error) {
    console.error('‚ùå Erro na an√°lise:', error);
    return {
      fileSize: 0,
      hasContent: false,
      quality: 'erro',
      likelyContent: 'n√£o identificado',
      technicalSuggestions: []
    };
  }
}

// üß† FUN√á√ÉO 3: DETECTAR TIPO DE CONTE√öDO AUDIO
function detectAudioContent(audioBuffer) {
  const size = audioBuffer.length;
  
  if (size < 10000) return 'muito_curto';
  if (size < 100000) return 'pergunta_rapida';
  if (size < 500000) return 'exemplo_musical_curto';
  if (size > 1000000) return 'faixa_completa';
  
  return 'conteudo_musical';
}

// ‚öôÔ∏è FUN√á√ÉO 4: GERAR SUGEST√ïES T√âCNICAS
function generateTechnicalSuggestions(audioBuffer) {
  const suggestions = [];
  const size = audioBuffer.length;
  
  if (size < 50000) {
    suggestions.push('√Åudio muito curto - grave por mais tempo para an√°lise melhor');
  }
  
  if (size > 2000000) {
    suggestions.push('√Åudio longo - considere enviar trechos espec√≠ficos para an√°lise focada');
  }
  
  suggestions.push('Para melhor an√°lise: grave em ambiente silencioso');
  suggestions.push('Posicione o microfone pr√≥ximo ao monitor/fone');
  
  return suggestions;
}

// ü§ñ FUN√á√ÉO 5: GERAR RESPOSTA DA IA
async function generateVoiceResponse(transcription, audioAnalysis, uid) {
  try {
    // Buscar perfil do usu√°rio para personaliza√ß√£o
    const userDoc = await db.collection('usuarios').doc(uid).get();
    const userProfile = userDoc.exists ? userDoc.data().perfil : {};

    // Prompt especializado para voice messages
    const voicePrompt = `Voc√™ √© o PROD.AI üéµ, especialista em produ√ß√£o musical. 

CONTEXTO: O usu√°rio enviou um VOICE MESSAGE (√°udio).

TRANSCRI√á√ÉO: "${transcription}"

AN√ÅLISE DO √ÅUDIO:
- Tamanho: ${audioAnalysis.fileSizeMB}MB
- Dura√ß√£o estimada: ~${audioAnalysis.estimatedDuration}s  
- Qualidade: ${audioAnalysis.quality}
- Tipo de conte√∫do: ${audioAnalysis.likelyContent}

PERFIL DO USU√ÅRIO:
- Estilo: ${userProfile.estilo || 'N√£o informado'}
- N√≠vel: ${userProfile.nivelTecnico || 'N√£o informado'} 
- DAW: ${userProfile.daw || 'N√£o informado'}

INSTRU√á√ïES ESPECIAIS PARA VOICE MESSAGE:
1. Responda como se voc√™ REALMENTE OUVIU o √°udio
2. Seja espec√≠fico sobre o que o usu√°rio falou
3. Se ele enviou √°udio musical, analise tecnicamente
4. Use emojis e seja direto
5. D√™ solu√ß√µes pr√°ticas imediatas
6. Valores exatos: Hz, dB, ms, etc.

FORMATO DA RESPOSTA:
üé§ [Confirma√ß√£o do que entendeu]
üéõÔ∏è [An√°lise t√©cnica do problema/situa√ß√£o]
üí° [Solu√ß√µes pr√°ticas com par√¢metros]
üöÄ [Pr√≥ximos passos]

Responda com excel√™ncia t√©cnica e personaliza√ß√£o total!`;

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
      },
      body: JSON.stringify({
        model: 'gpt-3.5-turbo',
        messages: [{
          role: 'system',
          content: voicePrompt
        }],
        temperature: 0.7,
        max_tokens: 1000
      })
    });

    if (!response.ok) {
      throw new Error(`OpenAI API erro: ${response.status}`);
    }

    const data = await response.json();
    return data.choices[0].message.content.trim();

  } catch (error) {
    console.error('‚ùå Erro na resposta da IA:', error);
    return `üé§ **√Åudio recebido com sucesso!**

üéõÔ∏è **An√°lise:** Detectei ${audioAnalysis.likelyContent} com qualidade ${audioAnalysis.quality}

üí° **Resposta baseada na transcri√ß√£o:** "${transcription}"

üöÄ **Continue enviando √°udios que posso ajudar ainda melhor com an√°lises t√©cnicas espec√≠ficas!**`;
  }
}

// üíæ FUN√á√ÉO 6: SALVAR INTERA√á√ÉO (ANALYTICS)
async function saveVoiceInteraction(uid, data) {
  try {
    await db.collection('voice_interactions').add({
      uid,
      ...data,
      createdAt: new Date()
    });
    console.log('üíæ Intera√ß√£o de voz salva');
  } catch (error) {
    console.error('‚ùå Erro ao salvar:', error);
    // N√£o falha a requisi√ß√£o se n√£o conseguir salvar
  }
}
